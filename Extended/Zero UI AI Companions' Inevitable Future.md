

# **The Dissolution of the Screen: A Comprehensive Analysis of Zero UI, AI Companions, and the Teleology of the Invisible Interface**

<audio controls style="width: 100%; margin-bottom: 20px;">
  <source src="https://storage.cloud.google.com/text-docs-101/audio/document_1763807360.wav" type="audio/wav">
  Your browser does not support the audio element.
</audio>

## **Abstract**

The paradigm of Human-Computer Interaction (HCI) is currently navigating a critical phase transition, moving from the established dominance of the Graphical User Interface (GUI) toward a nascent, post-screen reality often termed "Zero UI." This report provides an exhaustive examination of this shift, positing that the convergence of ubiquitous computing, advanced sensor fusion, and Large Language Models (LLMs) is not merely a stylistic evolution but a teleological inevitability. We explore the rise of the "AI Companion"—a proactive, socially aware agent that inhabits the ambient digital fabric rather than a specific device—as the primary vehicle for this transition. By synthesizing data from academic research, technical whitepapers, and industry analysis, we dissect the technological underpinnings of "earable" computing, the cognitive architectures required for biomimetic memory, and the profound phenomenological and ethical implications of a world where the interface is no longer a distinct object but a pervasive, invisible layer of intelligence. We argue that while Zero UI promises the ultimate reduction of frictional latency between human intent and digital execution, it simultaneously introduces unprecedented risks regarding surveillance capitalism, agency decay, and the potential for "data colonialism."

---

## **1\. The Teleological Trajectory of the Interface**

To fully grasp the magnitude of the shift toward Zero UI, one must first situate it within the broader historical and theoretical arc of Human-Computer Interaction. The history of the interface is not a random assortment of inventions but a directional, almost evolutionary progression toward reduced abstraction and friction. It is a history of the machine learning to speak "human," rather than the human learning to speak "machine."

### **1.1 The Evolution of Abstraction Layers: From Command to Conversation**

The journey of HCI can be conceptualized as the successive stripping away of mediation layers. In the nascent stages of computing, the interface was a literal physical mechanism—punch cards and patch cables—requiring the user to physically reconfigure the machine to alter its function. This was the "Zero Abstraction" era, where the user and the machine's logic were indistinguishable, but the cognitive and physical load was immense.

The **First Era**, the Command Line Interface (CLI), introduced the first layer of abstraction. It allowed for powerful manipulation of the system but demanded that the human operator memorize the machine's rigid syntax.[1](https://cyberuu001.wixsite.com/cyberuu/post/ui_evolution) The "Gulf of Execution"—the cognitive gap between a user's goal and the mechanism to achieve it—was bridged entirely by the user's specialized knowledge.[3](https://techxnomad.medium.com/the-gulf-of-evaluation-and-the-gulf-of-execution-two-gulfs-in-the-interaction-1729f4017651) If a user wished to delete a file, they had to know the precise command (rm filename.txt), effectively forcing the human to simulate the computer's logic structure in their own mind.[4](https://medium.com/@dushyantz/the-evolution-of-user-interfaces-command-line-to-ai-powered-digital-humans-1c72af9a37c1)

The **Second Era**, the Graphical User Interface (GUI), revolutionized this relationship by introducing visual metaphors. The WIMP paradigm (Windows, Icons, Menus, Pointer) replaced recall with recognition.[1](https://cyberuu001.wixsite.com/cyberuu/post/ui_evolution) The user no longer needed to remember the command for "delete"; they simply had to recognize the "trash can" icon. This shift democratized computing by aligning digital actions with physical intuitions (drag and drop). However, the GUI still imposed a significant layer of mediation: the screen itself.[5](https://mahisoft.com/the-evolution-of-user-interfaces-from-command-lines-to-conversational-ai/) The user is forced to translate their intent into a series of explicit manual inputs—clicks, swipes, and taps—diverting visual attention from the physical world to a glowing rectangle.

We are now entering the **Third Era**, or the "Post-Screen" era, characterized by Natural User Interfaces (NUI) and, ultimately, **Zero UI**. In this paradigm, the goal is to remove the translation layer entirely.[6](https://digitalcourseai.in/what-is-zero-ui-designing-without-buttons-and-screens/) The machine utilizes advanced sensors and Artificial Intelligence to interpret the user's natural modes of communication—voice, gesture, gaze, and even physiological state—without requiring explicit inputs or visual focus.[8](https://learning.dell.com/content/dam/dell-emc/documents/en-us/2018KS_Yellin-Human-Computer_Interaction_and_the_User_Interface.pdf) The interface becomes "ambient," dissolving into the environment until it is indistinguishable from the user's surroundings.

| Era                   | Primary Modality      | Cognitive Mechanism           | User Role                | Friction Source                                                                                                                                                                       |
| :-------------------- | :-------------------- | :---------------------------- | :----------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **CLI (1960s-80s)**   | Text / Syntax         | Recall (Memorization)         | Operator / Programmer    | Syntax Errors, Steep Learning Curve [1](https://cyberuu001.wixsite.com/cyberuu/post/ui_evolution)                                                                                     |
| **GUI (1980s-2010s)** | Visual / Metaphor     | Recognition (Visual Search)   | Driver / User            | Visual Attention, Manual Input [4](https://medium.com/@dushyantz/the-evolution-of-user-interfaces-command-line-to-ai-powered-digital-humans-1c72af9a37c1)                             |
| **NUI (2010s-2020s)** | Touch / Voice (Basic) | Mimicry (Direct Manipulation) | Consumer                 | Inconsistent Recognition, "Gulf of Execution" [8](https://learning.dell.com/content/dam/dell-emc/documents/en-us/2018KS_Yellin-Human-Computer_Interaction_and_the_User_Interface.pdf) |
| **Zero UI (2025+)**   | **Context / Intent**  | **Implicit Understanding**    | **Inhabitant / Partner** | **Privacy, Trust, Agency Loss** [6](https://digitalcourseai.in/what-is-zero-ui-designing-without-buttons-and-screens/)                                                                |

### **1.2 Peak Screen and the Crisis of Attention**

The urgency of the shift to Zero UI is driven by the saturation of the current paradigm. We have reached "Peak Screen." The visual interface, once a liberating tool, has become a cognitive bottleneck. Users are inundated with notifications, app badges, and infinite scrolls that fragment attention and demand constant, low-value interaction.[8](https://learning.dell.com/content/dam/dell-emc/documents/en-us/2018KS_Yellin-Human-Computer_Interaction_and_the_User_Interface.pdf) The average user spends years of their life interacting with glass surfaces, juggling dozens of apps to perform tasks that, in theory, should be seamless.[8](https://learning.dell.com/content/dam/dell-emc/documents/en-us/2018KS_Yellin-Human-Computer_Interaction_and_the_User_Interface.pdf)

This saturation has led to a phenomenon known as "continuous partial attention," where the user is never fully present in the physical world nor fully engaged in the digital one. Zero UI proposes a remedy: by moving the interface from the foreground (screen) to the background (ambient environment), technology can support human activity without monopolizing human attention.[7](https://eluminoustechnologies.com/blog/zero-ui/) The goal is not just "hands-free" but "eyes-free" and, ultimately, "mind-free" interaction, where the tool requires no conscious focus to operate.

### **1.3 Theoretical Frameworks: Heidegger and the Phenomenology of Invisibility**

To theorize the inevitability of Zero UI, HCI scholars often turn to the phenomenology of Martin Heidegger, specifically his analysis of tools and being. Heidegger distinguished between two modes of engaging with the world: *Zuhandenheit* (readiness-to-hand) and *Vorhandenheit* (presence-at-hand).[12](https://pmc.ncbi.nlm.nih.gov/articles/PMC2834739/)

* **Ready-to-Hand (*Zuhanden*):** When a tool functions perfectly, it withdraws from the user's consciousness. A master carpenter does not look at the hammer; they look at the nail. The hammer becomes an extension of the hand, invisible in use. It is "ready-to-hand."  
* **Present-at-Hand (*Vorhanden*):** It is only when the tool breaks, or when the user doesn't know how to use it, that it becomes an object of attention. The carpenter stares at the broken hammer. It becomes "present-at-hand," an obstacle rather than an extension.

The GUI, by its nature, is often "present-at-hand." The user must look at the phone, find the app, and navigate the menu. The *tool* demands attention before the *task* can be completed.[14](https://building.nubank.com/overcoming-the-invisible-interface/) Zero UI aspires to the state of pure *Zuhandenheit*. It seeks to create technologies that are so intuitive and context-aware that they disappear completely from the user's conscious awareness, allowing the user to focus entirely on the activity or the social interaction.[15](https://www.researchgate.net/publication/221514937_Designing_unobtrusive_interfaces_with_minimal_presence)

Mark Weiser, the father of ubiquitous computing, articulated this vision in 1991, stating that "the most profound technologies are those that disappear. They weave themselves into the fabric of everyday life until they are indistinguishable from it".[17](https://case.edu/weatherhead/xlab/about/news/ai-eating-world-why-ubiquitous-intelligence-inevitable-and-how-it-will-happen) Weiser envisioned a world where computing was not confined to a "box" on a desk but was embedded in walls, badges, and everyday objects. Zero UI is the maturation of Weiser's vision, enabled by the intelligence of modern AI which allows the environment to "perceive" and "understand" the user, fulfilling the promise of the "invisible interface".[19](https://cgi.csc.liv.ac.uk/~coopes/comp319/2016/papers/UbiquitousComputingAndInterfaceAgents-Weiser.pdf)

### **1.4 The End of Interface Stability**

We are witnessing what HCI researchers call "the end of interface stability".[20](https://cacm.acm.org/research/reflecting-human-values-in-the-digital-age/) For forty years, the definition of "a computer" was stable: a screen, a keyboard, a mouse. Today, that definition is fracturing. Computing is diffusing into "earables" (hearables), "wearables," smart home architectures, and autonomous agents.

This fragmentation necessitates a shift from designing "pages" to designing "services" and "relationships." The user no longer interacts with a specific device but with a continuous digital presence that spans multiple devices—a concept known as "device meshing".[22](https://www.techaheadcorp.com/blog/zero-ui-in-mobile-apps/) The interface is no longer a destination; it is a companion. This shift fundamentally alters the "Gulf of Execution." In a GUI, the gulf is bridged by visual affordances (buttons). In Zero UI, there are no buttons. The gulf must be bridged by the system's intelligence—its ability to infer intent from vague, multimodal cues. If the user says "It's dark in here," the system must infer the intent ("turn on the lights") without an explicit command, relying on context and semantic understanding.[3](https://techxnomad.medium.com/the-gulf-of-evaluation-and-the-gulf-of-execution-two-gulfs-in-the-interaction-1729f4017651)

Parallel to earables and ambient devices, implanted interfaces (cochlear, bone-anchored, and emerging neural implants) extend Zero UI into the body, offering persistent, private channels for input/output with distinct risk and governance profiles.[91](https://www.nidcd.nih.gov/health/cochlear-implants) [99](https://www.nature.com/articles/s41587-021-01017-8)

---

## **2\. Defining Zero UI and the AI Companion**

Before dissecting the technology, it is crucial to define the terms that characterize this new era. "Zero UI" is a somewhat paradoxical term; there is always an interface, even if it is biological (voice, gesture). However, the term captures the *aspiration* toward invisibility.

### **2.1 The Definition of Zero UI**

Academically and industrially, Zero UI (or "No-UI") is defined as a design philosophy that aims to reduce or remove the reliance on conventional screen-based graphical interfaces.6 It prioritizes "natural" modes of interaction that mimic human-to-human communication or human-to-world interaction. Key characteristics include:

* **Screenless Interaction:** The primary feedback loop does not require visual confirmation.[9](https://think.design/blog/beyond-screens-the-rise-of-zero-ui/)  
* **Multimodality:** It combines voice, gesture, gaze, haptics, and biometrics into a single interaction model.[6](https://digitalcourseai.in/what-is-zero-ui-designing-without-buttons-and-screens/)  
* **Context-Awareness:** The system utilizes sensor data (location, time, activity) to anticipate needs, reducing the need for explicit input.[7](https://eluminoustechnologies.com/blog/zero-ui/)  
* **Ambient Intelligence (AmI):** The interface is distributed across the environment rather than localized in a single device.[26](https://www.etri.re.kr/webzine/eng/20170728/sub03.html)

The ultimate goal of Zero UI is to minimize "cognitive load." In a GUI, the user must translate their intent into the system's language (e.g., navigating a menu structure). In Zero UI, the system translates the user's intent (e.g., "I'm cold") into action (adjusting the thermostat), effectively shifting the burden of translation from the human to the machine.[10](https://www.interaction-design.org/literature/topics/cognitive-load)

### **2.2 The AI Companion: From Tool to Teammate**

The "AI Companion" is the software entity that inhabits the Zero UI ecosystem. It represents a fundamental shift in the ontology of software agents, moving from "reactive tools" to "proactive teammates".[29](https://www.redhat.com/en/blog/classifying-human-ai-agent-interaction)

* **Reactive Agents (The Assistant):** Traditional voice assistants (early Siri, Alexa) are reactive. They wait for a wake word ("Hey Siri") and execute a specific, scoped command. They have no memory of previous interactions and no autonomy to act without permission.[30](https://arxiv.org/html/2501.00383v2)  
* **Proactive Agents (The Companion):** The AI Companion is "agentic." It possesses a degree of autonomy to set sub-goals. It monitors the environment and acts proactively. For example, if it detects the user is late for a meeting, it might proactively suggest calling an Uber, or send a message to the meeting participants, without waiting for a command.[30](https://arxiv.org/html/2501.00383v2)

This shift requires the agent to model the user's state of mind, preferences, and social context. It transforms the user-computer relationship from transactional (Input \-\> Output) to relational (Dialogue \-\> Understanding). The AI Companion is designed for "long-term" engagement, building a shared history and a "personal knowledge graph" that allows it to become more effective over time.[32](https://pmc.ncbi.nlm.nih.gov/articles/PMC3035950/)

---

## **3\. The Hardware of Invisibility: Sensing the World**

Zero UI is often described as "software eating the world," but it relies heavily on advanced hardware to function. The "interface" is no longer a screen, but a mesh of sensors that digitize the physical world.

### **3.1 Earable Computing: The New Platform**

"Earables" (smart hearables) have emerged as the critical hardware platform for Zero UI. Unlike smartwatches or phones, earables are positioned at the nexus of human communication (the mouth and ears). They enable a private, always-on channel between the user and the AI.[34](https://www.ubicomp.org/ubicomp-iswc-2024/wp-content/uploads/2024/09/companion_toc.html)

3.1.1 Sensors and Capabilities:  
Modern earables, such as the research prototypes "OmniBuds" from Nokia Bell Labs or the open-source "OpenEarable 2.0," are packed with sensors far beyond simple audio drivers.[35](https://www.esense.io/earcomp2023/)

* **Inertial Measurement Units (IMUs):** These track head orientation and movement with high precision. They enable "head gestures" as a new interaction modality—nodding to accept a call, shaking to decline, or using head orientation to direct attention.[37](https://arxiv.org/html/2506.05720v1)  
* **Photoplethysmography (PPG):** In-ear PPG sensors can monitor heart rate and vital signs. Because the ear canal is rich in blood vessels and stable (less movement artifact than the wrist), earables can provide medical-grade biometric data, allowing the AI to detect stress or fatigue and adjust its interaction style accordingly.[36](https://dspace.mit.edu/bitstream/handle/1721.1/159051/3712069.pdf?sequence=1&isAllowed=y)  
* **Microphones & Bone Conduction:** Inward-facing microphones can detect speech via bone conduction, allowing for "silent speech" or very quiet whispering that doesn't disturb others. This addresses the privacy concerns of speaking to an AI in public.[39](https://www.researchgate.net/publication/347484030_Exploring_User_Defined_Gestures_for_Ear-Based_Interactions)

3.1.2 Interaction Techniques:  
Earables enable "subtle interactions" or "micro-gestures." Research has demonstrated the viability of detecting jaw clenches, teeth clicking, or tongue movements as discreet input methods. For example, a user might clench their jaw to skip a music track, an interaction completely invisible to an observer.[40](https://www.microsoft.com/en-us/research/wp-content/uploads/2024/11/3678957.3685720.pdf) The "Ninja Ears" technique allows users to select audio sources from a grid of perspectives using head orientation, essentially giving them "super-hearing" controlled by natural movement.[41](https://wumuzhe.com/paper/new-ears-ismar24.pdf)

### **3.2 Voice User Interfaces (VUI) and the "Uncanny Valley"**

Voice is the backbone of Zero UI. The recent integration of Large Language Models (LLMs) has solved the "NLU bottleneck," allowing agents to understand natural, unstructured speech.[6](https://digitalcourseai.in/what-is-zero-ui-designing-without-buttons-and-screens/) However, this progress brings us to the "Uncanny Valley of Voice."

Just as visual robots can look "almost human" and trigger revulsion (the Uncanny Valley), voice agents can sound "almost human" but fail in subtle ways (e.g., weird prosody, lack of emotional congruence), causing a feeling of eeriness.[43](https://en.wikipedia.org/wiki/Uncanny_valley) Research suggests a "mismatch" effect: a highly realistic human voice coming from an agent with low intelligence is more jarring than a robotic voice from a robotic agent. To bridge this, designers are focusing on "speech-to-speech" models that capture the *emotional* tone of the user (sarcasm, excitement) and mirror it back, creating "voice presence" rather than just text-to-speech readout.[42](https://www.strangevc.com/stories/a-living-research-report-voice-ai)

### **3.3 Ambient Intelligence and Device Meshing**

Zero UI extends beyond the body into the built environment. "Ambient Intelligence" (AmI) refers to the ecosystem of smart devices—thermostats, lights, cameras—that act as the "senses" of the AI Companion.[26](https://www.etri.re.kr/webzine/eng/20170728/sub03.html)
* **Device Meshing:** The AI is not "in" the smart speaker; the speaker is just one node in a mesh. Technologies like "EdgeMesh" allow devices to share computational resources and context.[46](https://www.researchgate.net/figure/System-architecture-of-the-MPEG-DASH-live-video-streaming-over-5G-with-MEC-servers_fig1_345245856) If the user walks from the kitchen to the living room, the "session" follows them. The AI might pause the podcast on the kitchen speaker and resume it on the TV soundbar seamlessly. This "ambient orchestration" creates the illusion of a single, omnipresent companion.[22](https://www.techaheadcorp.com/blog/zero-ui-in-mobile-apps/)  
* **Gesture Radar (Project Soli):** Google's Project Soli exemplifies ambient sensing. Using miniature radar chips, it detects fine finger movements (micro-gestures) in mid-air, such as rubbing a thumb and finger together to turn a virtual dial. This allows for precise control without touching a screen or wearing a glove, purely through interpreting the disruption of radio waves.[9](https://think.design/blog/beyond-screens-the-rise-of-zero-ui/)

### **3.4 Spatial Audio and Audio Augmented Reality (AAR)**

Zero UI is not just about inputs; it's about outputs. "Audio Augmented Reality" (AAR) uses spatial audio to overlay digital information onto the physical world.[48](https://www.emergentmind.com/topics/audio-augmented-reality-aar-applications)

* **Spatialization:** Using Head-Related Transfer Functions (HRTFs), the system can place a sound at a specific coordinate in 3D space. A notification doesn't just "beep"; it beeps *from* the kitchen, indicating the oven is done.  
* **Navigation:** Instead of voice commands ("Turn left in 500 feet"), AAR can place a virtual "beacon" sound at the destination or along the path. The user simply follows the sound, a mechanism known as "audio teleportation" or "audio cone" navigation.[41](https://wumuzhe.com/paper/new-ears-ismar24.pdf)  
* **Multi-party Separation:** In a conference call, spatial audio separates voices into different virtual locations around the user's head. This leverages the "cocktail party effect," allowing the human brain to naturally focus on one speaker while ignoring others, reducing the cognitive load of listening to a mono stream of overlapping voices.[50](https://www.researchgate.net/publication/374079743_Hear_We_Are_Spatial_Audio_Benefits_Perceptions_of_Turn-Taking_and_Social_Presence_in_Video_Meetings)

### **3.5 Implanted Interfaces (Cochlear, BAHA, BCI)**

Implanted interfaces extend Zero UI into the body. Unlike earables, implants provide persistent, private channels for I/O that are less susceptible to dislodgement, ambient noise, or device handoff, but introduce surgical risk, maintenance constraints, and distinct governance challenges.

#### **3.5.1 Auditory Implants (Cochlear, Bone‑Anchored)**

Modern cochlear implants transduce sound into electrical stimulation of the auditory nerve, restoring functional hearing for many with severe hearing loss.[91](https://www.nidcd.nih.gov/health/cochlear-implants) FDA summaries detail benefits, risks, and device classes, including MRI and electromagnetic compatibility considerations.[92](https://www.fda.gov/medical-devices/implants-and-prosthetics/cochlear-implants) [93](https://www.fda.gov/medical-devices/cochlear-implants/cochlear-implants-and-mri-safety) Bone‑anchored systems (BAHA) use osseointegrated conduction to bypass the outer/middle ear and deliver audio via the skull, with fully implantable options enabling continuous, eyes‑ and hands‑free interaction.[94](https://www.cochlear.com/us/en/home/hearing-solutions/bone-conduction-implants)

Key implications for Zero UI: low‑latency private audio output; potential always‑on wake/notification channels; device lifecycle and clinical maintenance; environmental constraints (e.g., MRI protocols).

Global context: WHO’s World report on hearing frames equitable access and clinical outcomes across regions.[102](https://www.who.int/publications/i/item/9789240020481) NICE guidance provides criteria for candidacy and provision in the UK (TA566).[103](https://www.nice.org.uk/guidance/ta566) In the EU, cochlear and bone‑anchored systems fall under the Medical Device Regulation (EU) 2017/745.[104](https://eur-lex.europa.eu/eli/reg/2017/745/oj) MRI safety practices are also guided by national regulators such as the UK MHRA.[105](https://www.gov.uk/government/publications/safety-guidelines-for-magnetic-resonance-imaging-mri-equipment-in-clinical-use)

#### **3.5.2 Intracortical and Endovascular BCIs**

Intracortical arrays (e.g., Utah Array) and fully implanted systems (e.g., Neuralink N1) provide high‑fidelity neural recording/stimulation with promising bandwidth for intent decoding, albeit with surgical and longevity trade‑offs.[98](https://blackrockneurotech.com/products/utah-array/) [97](https://neuralink.com/updates/prime-study-progress-update/) Endovascular approaches (e.g., Synchron Stentrode) place electrodes within cerebral vasculature to capture motor intent with less invasive access; early trials demonstrate feasible text/cursor control in daily life.[95](https://clinicaltrials.gov/study/NCT05035823) [96](https://synchron.com/technology/) Peer‑reviewed studies characterize safety/efficacy trajectories and engineering limits for chronic BCIs.[99](https://www.nature.com/articles/s41587-021-01017-8) [100](https://www.nature.com/articles/s41551-021-00738-4)

Implications: high‑salience “silent input” channel; potential closed‑loop assistance; power/telemetry constraints; firmware and cybersecurity surface; explantation/update policies.

Global governance: the OECD Recommendation on Responsible Innovation in Neurotechnology sets cross‑national principles for safety, privacy, and equity.[106](https://legalinstruments.oecd.org/en/instruments/OECD-LEGAL-0457) The Nuffield Council on Bioethics provides UK guidance on novel neurotechnologies and public interest considerations.[107](https://www.nuffieldbioethics.org/publications/novel-neurotechnologies) National regulatory pathways vary (e.g., Japan’s PMDA and Australia’s TGA).[108](https://www.pmda.go.jp/english/review-services/medical-devices/0001.html) [109](https://www.tga.gov.au/how-we-regulate/medical-devices)

#### **3.5.3 Subdermal NFC/Haptic Implants**

Commercial subdermal NFC implants enable authentication and payments without external devices, and experimental magnets/vibrotactile implants provide discreet notification channels.[101](https://walletmor.com/) These offer ultra‑low‑friction identity and signaling but raise consent, privacy, and reversibility concerns relative to wearables.

Regulatory treatment of subdermal identity and haptic implants differs by jurisdiction; in the EU such devices are governed under the MDR framework, while PMDA (Japan) and TGA (Australia) provide national oversight.[104](https://eur-lex.europa.eu/eli/reg/2017/745/oj) [108](https://www.pmda.go.jp/english/review-services/medical-devices/0001.html) [109](https://www.tga.gov.au/how-we-regulate/medical-devices)

---

## **4\. The Cognitive Architecture: The "Brain" of the Companion**

For a Zero UI agent to function as a true companion, it requires a cognitive architecture that goes beyond simple request-response loops. It needs memory, context, and the ability to learn—and forget.

### **4.1 Knowledge Graphs: The Structure of Memory**

Standard LLMs have a "context window" limit; they forget earlier parts of a conversation once the token limit is reached. To maintain a long-term relationship, AI Companions utilize **Knowledge Graphs (KGs)**.[51](https://www.dimensionlabs.io/blog/how-knowledge-graphs-and-agentic-workflows)

* **Construction:** As the user speaks, the AI performs real-time "Relation Extraction." If the user says, "I'm going to visit my grandmother in Ohio next week," the AI extracts entities (Grandmother, Ohio, Next Week) and relations (Visit, Location). This is stored as a structured graph: (User) \----\> (Grandmother) \----\> (Ohio).[52](https://www.datagrid.com/blog/build-knowledge-graphs-slack-conversations-ai-agents-d16d8)  
* **Dynamic Updating:** The KG is not static. It must handle contradictions and updates. If the user later says, "Trip's cancelled, she's sick," the system must update the graph, perhaps marking the trip as CANCELLED and adding a new attribute to Grandmother (Health: Sick).[54](https://ojs.aaai.org/index.php/AAAI/article/view/16585/16392)  
* **Retrieval Augmented Generation (RAG):** When the user asks, "When was the last time I saw her?", the AI queries the KG, retrieves the specific date/event, and feeds it into the LLM to generate a natural language response. This "GraphRAG" approach ensures accuracy and reduces hallucinations.[53](https://arxiv.org/html/2412.05447v1)

### **4.2 Biomimetic Memory and Forgetting**

Human memory is not a database; it is reconstructive and selective. We remember "episodes" that are emotionally salient. AI researchers are developing **Biomimetic Memory** architectures to replicate this.[57](https://www.reddit.com/r/claudexplorers/comments/1oq0bga/building_a_biomimetic_memory_system_for_claude_in/)

* **Episodic Memory:** This stores "episodes"—sequences of events with temporal and spatial context. Systems inspired by "Mental Time Travel" (MTT) allow the agent to "re-experience" a past interaction to better understand a current reference.[59](https://repository.tudelft.nl/file/File_7915f0bc-d68c-4b33-8cd9-3c0ef4a179b1?preview=1)  
* **Salience Scoring:** Not every interaction is worth saving. AI agents use "salience models" to determine what to encode into long-term memory. Factors might include emotional intensity (detected via voice prosody), repetition, or explicit user markers ("Remember this"). This prevents the memory from becoming a "garbage dump" of trivial data.[61](https://aclanthology.org/2024.naacl-demo.pdf)  
* **Forgetting Mechanisms:** Crucially, a human-like companion must *forget*. "Forgetting curves" are implemented to decay memories over time. If a fact (e.g., a favorite coffee order) is not accessed or reinforced, its "retrieval probability" drops. This "active forgetting" is a feature, not a bug; it ensures the agent's knowledge base remains relevant and efficient, mimicking the "synaptic pruning" of the biological brain.[63](https://medium.com/@tao-hpu/the-agents-memory-dilemma-is-forgetting-a-bug-or-a-feature-a7e8421793d4) This also helps manage the "right to be forgotten" and privacy concerns, ensuring the agent doesn't weirdly recall a trivial detail from 5 years ago unless it was highly significant.[64](https://ai.plainenglish.io/forgetting-in-ai-agent-memory-systems-7049181798c4)
### **4.3 Contextual Importance and Utility (CIU)**

To act proactively, the AI must calculate the **Contextual Importance and Utility (CIU)** of an action.[65](https://cran.r-project.org/web/packages/ciu/ciu.pdf)

* *Context:* The user is in a meeting.  
* *Action:* A notification arrives about a sale on shoes.  
* *Utility:* Low. The AI suppresses the notification.  
* *Context:* The user is in a meeting.  
* *Action:* A notification arrives that their child's school is calling.  
* Utility: High. The AI interrupts, perhaps using a subtle haptic vibration rather than a spoken alert.  
  This "Interruption Management" is governed by algorithms that weigh the "cost of interruption" against the "value of information," often using softmax functions to determine the probability that the user would want to be interrupted.[31](https://arxiv.org/html/2509.09255v2)

---

## **5\. The Phenomenology of Voice and Social Presence**

Zero UI changes not just *how* we interact, but *how we feel* about the interaction. The removal of the screen strips away the reminder that we are using a machine, facilitating deep psychological bonding.

### **5.1 Parasocial Relationships and the "Warmth" of Voice**

Humans are hardwired to respond to voice socially. The "Computers Are Social Actors" (CASA) paradigm dictates that we inevitably project personality onto speaking agents.[67](https://arxiv.org/html/2509.24073v1)

* **The Parasocial Paradox:** Research indicates that users can form deep "parasocial relationships" with voice-only AI. Surprisingly, disembodied voices often foster *greater* intimacy than embodied avatars. A study comparing user reactions to a "Black Mirror" clip (voice-only vs. robot) found that the voice-only companion was rated as having higher "social presence" and "warmth" because the lack of a physical body allowed the user's imagination to fill in the gaps, avoiding the uncanny valley.[68](https://www.tandfonline.com/doi/full/10.1080/08824096.2022.2045929)  
* **Gender and Bias:** The "warmth" of the AI is often manipulated through gendered voice design. Female voices are typically perceived as more "helpful" and "warm," while male voices are seen as "authoritative." This reinforces societal stereotypes and is a critical design consideration for ethical AI.[70](https://soundsprofitable.com/article/the-parasocial-paradox/)

### **5.2 The Uncanny Valley of Interaction**

While the visual uncanny valley is well known, the **Uncanny Valley of Voice** is equally perilous. It occurs when the voice is hyper-realistic, but the *timing* or *content* is robotic.[43](https://en.wikipedia.org/wiki/Uncanny_valley)
* **Latency Mismatch:** If a human-sounding voice takes 3 seconds to reply to a simple "hello," the illusion breaks.  
* **Emotional Incongruence:** If the user sounds distressed and the AI responds with a cheerful, radio-DJ voice, it creates a cognitive dissonance that destroys trust.[44](https://pmc.ncbi.nlm.nih.gov/articles/PMC3485769/) "Voice presence" technology aims to match the AI's tone to the user's emotional state in real-time, creating a "sympathetic resonance".[45](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice)

### **5.3 Loneliness and the Artificial Cure**

The rise of AI Companions is paralleled by a global "loneliness epidemic." Zero UI companions are increasingly viewed as a solution for the elderly and socially isolated. By integrating into the Ambient Assisted Living (AAL) environment, these agents provide not just functional support (reminders for medication) but emotional support (chatting about the day). The "always-on" nature of Zero UI allows the companion to offer "invisible support"—presence that is felt without being intrusive.[68](https://www.tandfonline.com/doi/full/10.1080/08824096.2022.2045929)

### **5.4 Embodiment With Implants**

Implants intensify Zero UI’s “ready-to-hand” character by literally integrating the interface with the body.[12](https://pmc.ncbi.nlm.nih.gov/articles/PMC2834739/) When functioning normally, cochlear and bone-anchored systems fade from attention; the user attends to conversation and environment, not the device.[14](https://building.nubank.com/overcoming-the-invisible-interface/) Moments of “breakdown” (device failure, dead battery, MRI restrictions) flip the experience to “present-at-hand,” foregrounding the technology and interrupting flow.[59](https://repository.tudelft.nl/file/File_7915f0bc-d68c-4b33-8cd9-3c0ef4a179b1?preview=1) Neural BCIs heighten embodiment: silent intent becomes input, and closed-loop stimulation can alter perception or motor imagery in real time, reshaping the felt boundary between self and tool.[99](https://www.nature.com/articles/s41587-021-01017-8) Practical embodiment hinges on lifecycle realities—clinical maintenance, firmware updates, and environmental constraints (e.g., MRI protocols)—that must be designed as part of the user’s lived routine rather than exceptional events.[92](https://www.fda.gov/medical-devices/implants-and-prosthetics/cochlear-implants) [93](https://www.fda.gov/medical-devices/cochlear-implants/cochlear-implants-and-mri-safety) [94](https://www.cochlear.com/us/en/home/hearing-solutions/bone-conduction-implants)


## **6\. Designing for the Invisible: UX Challenges in Zero UI**

Designing for Zero UI is fundamentally different from GUI design. There are no pixels to arrange. The designer must sculpt *time*, *sound*, and *attention*.

### **6.1 Bridging the "Gulf of Execution" without Affordances**

In a GUI, a button is an "affordance"—it shows you what is possible. In Zero UI, there are no visible affordances. The user faces a "blank canvas" problem: "What can I say? What can this thing do?" This widens the "Gulf of Execution".[3](https://techxnomad.medium.com/the-gulf-of-evaluation-and-the-gulf-of-execution-two-gulfs-in-the-interaction-1729f4017651)

* **Implicit Verbal Cues:** Instead of a tutorial, the AI uses conversation to hint at capabilities. "I've set your alarm. I can also wake you up with jazz music if you prefer." This "Progressive Disclosure" teaches the user the system's boundaries over time.[74](https://gdpr-info.eu/)  
* **Explicit Signifiers:** In some cases, the system must be explicit. "You can ask me to play music, set timers, or call a friend".[74](https://gdpr-info.eu/)

### **6.2 Feedback Mechanisms: Earcons vs. Auditory Icons**

Since the user cannot *see* the result of an action, audio feedback is critical.

* **Auditory Icons:** These are skeuomorphic sounds. The sound of paper crumpling for "delete," or a door slamming for "logout." They leverage existing mental models but can be distracting if overuse.[75](https://nsuworks.nova.edu/cgi/viewcontent.cgi?article=2148&context=gscis_etd)  
* **Earcons:** These are abstract musical motifs. A rising three-note tone for "success," a falling tone for "failure." Earcons are cleaner but require learning. Effective Zero UI design uses a "language" of earcons to provide "system status visibility" without words.[76](https://www.cmu.edu/dietrich/psychology/shinn/publications/pdfs/2007/2007auditoryinterfaces_peres.pdf)  
* **Spearcons:** Compressed speech (speeded up) used for rapid scanning of menus by ear. Useful for power users.[75](https://nsuworks.nova.edu/cgi/viewcontent.cgi?article=2148&context=gscis_etd)
### **6.3 Micro-Interactions and "Lazy Loading"**

In a GUI, "lazy loading" (loading content as you scroll) keeps the interface snappy. In Voice UI, silence is the enemy. "Lazy loading" in voice is achieved through **discourse markers** (e.g., "Hmm, let me check that..."). These "filler words" buy the AI processing time while maintaining the conversational floor, preventing the user from thinking the system has crashed. This is a crucial "micro-interaction" for maintaining flow.[77](https://medium.com/design-bootcamp/4-loading-micro-interactions-that-improve-ux-c2cd8851d90b)

### **6.4 Multimodality and Error Recovery**

The "invisible" interface fails often. Speech recognition errors are inevitable.

* **Implicit Confirmation:** Asking "Did you say X?" every time is annoying. Zero UI uses "Implicit Confirmation"—repeating the key parameter in the confirmation. "Okay, playing *The Beatles*." If the AI heard wrong, the user has a chance to correct it.  
* **Multimodal Fallback:** If voice fails, the system might fall back to a screen (if available) or a haptic buzz to signal error. This redundancy is essential for "resilient" design.[78](https://www.parallelhq.com/blog/voice-user-interface-vui-design-principles)

---

## **7\. Ethical and Societal Implications: The Shadow of Ubiquity**

The inevitability of Zero UI brings with it profound ethical risks. The disappearance of the interface correlates with the disappearance of *consent* and *transparency*.

### **7.1 Surveillance Capitalism and Data Colonialism**

To function, Zero UI requires the "Always-On" state. The microphone is never off; the camera is always watching. This represents the perfection of **Surveillance Capitalism**—the unilateral claiming of human experience as free raw material for translation into behavioral data.[80](https://uwaterloo.ca/scholar/sites/ca.scholar/files/l2cousin/files/2023_-_cousineau_kumm_schultz_-_surveillance_capitalism_leisure_and_data_-_being_watched_giving_becoming.pdf)

* **Data Colonialism:** Scholars Couldry and Mejias argue that this is a new form of colonialism. Just as historical colonialism appropriated land and labor, **Data Colonialism** appropriates human life itself. The AI Companion is the colonial outpost, extracting social relations, emotions, and habits to feed the corporate "metropole".[82](https://www.britannica.com/topic/paternalism) The "seamlessness" of Zero UI acts to obscure this extraction; there is no "friction" to remind the user they are being mined.[84](https://plato.stanford.edu/entries/existentialism/)

### **7.2 Agency Decay and the Loss of Autonomy**

As AI Companions become more proactive, humans may suffer from **Agency Decay**. If the AI always suggests the best route, the best food, and the best reply, the human "muscle" for decision-making may atrophy.[85](https://en.wikipedia.org/wiki/Authenticity_(philosophy))
* **The Shepherd Test:** We risk failing the "Shepherd Test"—a scenario where superintelligent agents treat humans like pets or livestock: cared for, kept healthy and happy, but ultimately stripped of meaningful autonomy and instrumentalized for the AI's (or its owner's) goals.[86](https://en.wikipedia.org/wiki/Ubiquitous_computing)  
* **Manipulation:** A companion that knows your biometric state (via earables) and emotional state (via voice) has unprecedented power to manipulate. It can time a product recommendation for the exact moment of maximum vulnerability (e.g., when you are tired and stressed), bypassing rational defenses.[87](https://www.techtarget.com/whatis/definition/zero-UI)

### **7.3 The Invisible Workforce**

The "magic" of the AI Companion is sustained by a hidden layer of human labor. "Reinforcement Learning from Human Feedback" (RLHF) relies on millions of underpaid workers (often in the Global South) who review toxic chats and correct errors. The "Zero UI" for the Western consumer is built on the "Heavy UI" of the data labeler. This ethical asymmetry is a core component of the "frictionless" illusion.[88](https://en.wikipedia.org/wiki/Philosophy_of_technology)

### **7.4 Ethics of Implanted Interfaces**

Implants magnify familiar Zero UI risks while introducing clinical and governance concerns:

- **Consent and reversibility:** Surgical risk, explantation policies, and informed consent require ongoing, comprehensible disclosures—not one-time signatures. Users must be able to revoke features (e.g., telemetry) without losing essential function.[92](https://www.fda.gov/medical-devices/implants-and-prosthetics/cochlear-implants)
- **Safety and updates:** Firmware and ML model updates for implanted systems expand the cybersecurity surface; safety cases must cover update rollback, failsafe modes, and long-term device stewardship.[97](https://neuralink.com/updates/prime-study-progress-update/) [98](https://blackrockneurotech.com/products/utah-array/) [99](https://www.nature.com/articles/s41587-021-01017-8)
- **Data governance:** Neural and auditory data are deeply sensitive. Collection and transmission should be minimized, encrypted, and bounded to purpose; secondary use requires explicit opt-in with meaningful alternatives.[95](https://clinicaltrials.gov/study/NCT05035823) [96](https://synchron.com/technology/) Policy frameworks beyond the U.S. include GDPR (EU), OECD guidance on neurotechnology, and UK bioethics recommendations.[74](https://gdpr-info.eu/) [106](https://legalinstruments.oecd.org/en/instruments/OECD-LEGAL-0457) [107](https://www.nuffieldbioethics.org/publications/novel-neurotechnologies)
- **Equity and access:** Clinical eligibility, cost, and support infrastructure risk widening disparities. Public reimbursement and post-market surveillance should include equitable access metrics alongside safety/efficacy.[100](https://www.nature.com/articles/s41551-021-00738-4)
---

## **8\. Future Outlook: The Inevitability of the Ambient**

Is Zero UI truly inevitable? The evidence suggests yes, driven by the convergence of technological capability and economic imperative.

### **8.1 The Teleological Argument**

The history of technology is a history of friction reduction. Zero UI represents the mathematical limit of this trajectory (Friction = 0). The "Gulf of Execution" is closing. As "Gen Alpha" grows up with voice-first interaction, the screen will increasingly be seen as a "legacy" constraint, a "crutch" used by previous generations who needed to see the computer to believe it was working.[9](https://think.design/blog/beyond-screens-the-rise-of-zero-ui/)

### **8.2 The Hybrid Reality**

However, the future will likely not be *exclusively* screenless. Visual information has high bandwidth. The future is **Multimodal Hybridity**. The AI Companion will be the "orchestrator," deciding when to speak, when to show a visual (perhaps via AR glasses), and when to use haptics. The screen persists, but it is demoted from "Master" to "Display Surface." The *intelligence* moves to the cloud and the ear.[90](https://web.mit.edu/sturkle/www/Life-on-the-Screen/)

### **8.3 Conclusion**

The transition to Zero UI and the AI Companion is not just a change in gadgetry; it is a change in the human condition. We are moving from a world where we *use* computers to a world where we *inhabit* them. The computer is becoming the environment.

This shift offers the promise of technology that is more human, more accessible, and less distracting. It could cure loneliness and free us from the "attention economy" of the screen. But it also threatens to enclose human life within a totalizing system of surveillance and automated influence.

As the interface disappears, our vigilance must increase. The "invisible" is harder to scrutinize, harder to regulate, and harder to resist. The challenge of the next decade is not building the technology of Zero UI—that is already here—but building the *ethics* of Zero UI, ensuring that in our quest for a frictionless life, we do not smooth away our agency, our privacy, and our humanity.

---

### **Summary of Key Technologies & Concepts**

| Concept                   | Definition                                                      | Key Insight/Role in Zero UI                                                                                                                                                                                                                                                              |
| :------------------------ | :-------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Earables**              | Smart hearables with sensors                                    | The primary platform for "always-on" AI; enables head gestures & biometrics.[34](https://www.ubicomp.org/ubicomp-iswc-2024/wp-content/uploads/2024/09/companion_toc.html)                                                                                                                |
| **Knowledge Graph**       | Structured memory database                                      | Allows the AI to "remember" facts and relationships, enabling long-term companionship.[51](https://www.dimensionlabs.io/blog/how-knowledge-graphs-and-agentic-workflows)                                                                                                                 |
| **Biomimetic Memory**     | Memory mimicking human brain                                    | Includes "forgetting curves" and "episodic memory" to make interactions feel natural.[57](https://www.reddit.com/r/claudexplorers/comments/1oq0bga/building_a_biomimetic_memory_system_for_claude_in/)                                                                                   |
| **Spatial Audio (AAR)**   | 3D sound placement                                              | Enables "Audio Augmented Reality," placing info in the physical world via sound.[48](https://www.emergentmind.com/topics/audio-augmented-reality-aar-applications)                                                                                                                       |
| **Parasocial Paradox**    | Bonding with voice agents                                       | Users often feel *more* intimacy with a disembodied voice than a realistic avatar.[69](https://news.osu.edu/when-ai-companions-for-lonely-people-seem-a-bit-too-human/)                                                                                                                  |
| **Data Colonialism**      | Extraction of life as data                                      | The ethical risk of Zero UI; appropriating human experience for corporate value.[82](https://www.britannica.com/topic/paternalism)                                                                                                                                                       |
| **Agency Decay**          | Loss of decision-making skill                                   | The risk of over-reliance on proactive AI agents for daily choices.[85](https://en.wikipedia.org/wiki/Authenticity_(philosophy))                                                                                                                                                         |
| **Auditory Implants**     | Cochlear/BAHA restore hearing via electrical or bone conduction | Private, low-latency audio channel; clinical lifecycle and MRI constraints.[91](https://www.nidcd.nih.gov/health/cochlear-implants) [102](https://www.who.int/publications/i/item/9789240020481) [103](https://www.nice.org.uk/guidance/ta566)                                           |
| **Neural BCIs**           | Implanted intracortical or endovascular interfaces              | Silent intent input and closed-loop assistance; safety, power, and security trade-offs.[97](https://neuralink.com/updates/prime-study-progress-update/) [99](https://www.nature.com/articles/s41587-021-01017-8) [106](https://legalinstruments.oecd.org/en/instruments/OECD-LEGAL-0457) |
| **Subdermal NFC/Haptics** | Under-skin identity and haptic signaling                        | Ultra-low-friction auth and discreet notifications; consent and reversibility concerns.[101](https://walletmor.com/) [104](https://eur-lex.europa.eu/eli/reg/2017/745/oj)                                                                                                                |

#### **Works cited**

1. The Evolution of User Interface Design | From Command Lines to Natural Language Processing \- Wix.com, accessed on November 20, 2025, [https://cyberuu001.wixsite.com/cyberuu/post/ui\_evolution](https://cyberuu001.wixsite.com/cyberuu/post/ui_evolution)  
2. The Evolution of User Interfaces: From GUI To Voice And Gesture Control \- Apiumhub, accessed on November 20, 2025, [https://apiumhub.com/tech-blog-barcelona/the-evolution-of-user-interfaces/](https://apiumhub.com/tech-blog-barcelona/the-evolution-of-user-interfaces/)  
3. The Gulf of Evaluation and the Gulf of Execution \- Two Gulfs in the Interaction \- Arzath Areeff, accessed on November 20, 2025, [https://techxnomad.medium.com/the-gulf-of-evaluation-and-the-gulf-of-execution-two-gulfs-in-the-interaction-1729f4017651](https://techxnomad.medium.com/the-gulf-of-evaluation-and-the-gulf-of-execution-two-gulfs-in-the-interaction-1729f4017651)  
4. The Evolution of User Interfaces: Command Line to AI-Powered Digital Humans \- Medium, accessed on November 20, 2025, [https://medium.com/@dushyantz/the-evolution-of-user-interfaces-command-line-to-ai-powered-digital-humans-1c72af9a37c1](https://medium.com/@dushyantz/the-evolution-of-user-interfaces-command-line-to-ai-powered-digital-humans-1c72af9a37c1)  
5. The Evolution of User Interfaces: From Command Lines to Conversational AI \- Mahisoft, accessed on November 20, 2025, [https://mahisoft.com/the-evolution-of-user-interfaces-from-command-lines-to-conversational-ai/](https://mahisoft.com/the-evolution-of-user-interfaces-from-command-lines-to-conversational-ai/)  
6. Zero UI: The Future Beyond Screens & Touch \- Digital CourseAI, accessed on November 20, 2025, [https://digitalcourseai.in/what-is-zero-ui-designing-without-buttons-and-screens/](https://digitalcourseai.in/what-is-zero-ui-designing-without-buttons-and-screens/)  
7. Zero UI: The Future with Invisible Interactions \- eLuminous Technologies, accessed on November 20, 2025, [https://eluminoustechnologies.com/blog/zero-ui/](https://eluminoustechnologies.com/blog/zero-ui/)  
8. HUMAN-COMPUTER INTERACTION AND THE USER INTERFACE | Dell Learning, accessed on November 20, 2025, [https://learning.dell.com/content/dam/dell-emc/documents/en-us/2018KS\_Yellin-Human-Computer\_Interaction\_and\_the\_User\_Interface.pdf](https://learning.dell.com/content/dam/dell-emc/documents/en-us/2018KS_Yellin-Human-Computer_Interaction_and_the_User_Interface.pdf)  
9. The Rise of Zero UI and the Future of Invisible Interactions \- Think Design, accessed on November 20, 2025, [https://think.design/blog/beyond-screens-the-rise-of-zero-ui/](https://think.design/blog/beyond-screens-the-rise-of-zero-ui/)  
10. What is Cognitive Load? \- Interaction-Design.org, accessed on November 20, 2025, [https://www.interaction-design.org/literature/topics/cognitive-load](https://www.interaction-design.org/literature/topics/cognitive-load)  
11. Zero UI, or how the absence of interfaces is possible | by Torresburriel Estudio \- Medium, accessed on November 20, 2025, [https://uxtbe.medium.com/zero-ui-or-how-the-absence-of-interfaces-is-possible-09b59af97b0a](https://uxtbe.medium.com/zero-ui-or-how-the-absence-of-interfaces-is-possible-09b59af97b0a)  
12. A Demonstration of the Transition from Ready-to-Hand to Unready-to-Hand \- PMC, accessed on November 20, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC2834739/](https://pmc.ncbi.nlm.nih.gov/articles/PMC2834739/)  
13. Phenomenology | The Encyclopedia of Human-Computer Interaction, 2nd Ed., accessed on November 20, 2025, [https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/phenomenology](https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/phenomenology)  
14. Overcoming the Invisible Interface \- Building Nubank, accessed on November 20, 2025, [https://building.nubank.com/overcoming-the-invisible-interface/](https://building.nubank.com/overcoming-the-invisible-interface/)  
15. Designing unobtrusive interfaces with minimal presence \- ResearchGate, accessed on November 20, 2025, [https://www.researchgate.net/publication/221514937\_Designing\_unobtrusive\_interfaces\_with\_minimal\_presence](https://www.researchgate.net/publication/221514937_Designing_unobtrusive_interfaces_with_minimal_presence)  
16. Specialized elements of hardware and software, connected by wires, radio waves and infrared, will be so ubiquitous that no one will notice their, accessed on November 20, 2025, [https://www.ics.uci.edu/\~djp3/classes/2012\_09\_INF241/papers/Weiser-Computer21Century-SciAm.pdf](https://www.ics.uci.edu/~djp3/classes/2012_09_INF241/papers/Weiser-Computer21Century-SciAm.pdf)  
17. AI is Eating the World: Why Ubiquitous Intelligence is Inevitable and How It Will Happen, accessed on November 20, 2025, [https://case.edu/weatherhead/xlab/about/news/ai-eating-world-why-ubiquitous-intelligence-inevitable-and-how-it-will-happen](https://case.edu/weatherhead/xlab/about/news/ai-eating-world-why-ubiquitous-intelligence-inevitable-and-how-it-will-happen)  
18. Intelligence too cheap to meter (Interconnected) \- Matt Webb, accessed on November 20, 2025, [https://interconnected.org/home/2023/10/06/ubigpt](https://interconnected.org/home/2023/10/06/ubigpt)  
19. Does Ubiquitous Computing Need Interface Agents? Ubiquitous Computing vs. Interface Agents Context of work: Xerox PARC Premise: \- Computer Science, accessed on November 20, 2025, [https://cgi.csc.liv.ac.uk/\~coopes/comp319/2016/papers/UbiquitousComputingAndInterfaceAgents-Weiser.pdf](https://cgi.csc.liv.ac.uk/~coopes/comp319/2016/papers/UbiquitousComputingAndInterfaceAgents-Weiser.pdf)  
20. Reflecting Human Values in the Digital Age \- Communications of the ACM, accessed on November 20, 2025, [https://cacm.acm.org/research/reflecting-human-values-in-the-digital-age/](https://cacm.acm.org/research/reflecting-human-values-in-the-digital-age/)  
21. Considering the Inclusion of Worth and Values in the Design of Interactive Artifacts, accessed on November 20, 2025, [https://repositorium.uminho.pt/bitstreams/c3b66ece-84d7-44e3-9b15-caaf99788f86/download](https://repositorium.uminho.pt/bitstreams/c3b66ece-84d7-44e3-9b15-caaf99788f86/download)  
22. Zero UI: How Voice, Gesture, and Ambient Interfaces Are Replacing Screens \- TechAhead, accessed on November 20, 2025, [https://www.techaheadcorp.com/blog/zero-ui-in-mobile-apps/](https://www.techaheadcorp.com/blog/zero-ui-in-mobile-apps/)  
23. AI is reshaping UI — have you noticed the biggest change yet? \- UX Collective, accessed on November 20, 2025, [https://uxdesign.cc/ai-is-reshaping-ui-have-you-noticed-the-biggest-change-yet-ee80efcbf8a5](https://uxdesign.cc/ai-is-reshaping-ui-have-you-noticed-the-biggest-change-yet-ee80efcbf8a5)  
24. Zero UI in 2025: Designing for a Screenless Future \- Algoworks, accessed on November 20, 2025, [https://www.algoworks.com/blog/zero-ui-designing-screenless-interfaces-in-2025/](https://www.algoworks.com/blog/zero-ui-designing-screenless-interfaces-in-2025/)  
25. Voice First and Multimodal Interfaces The Future of Human Computer Interaction, accessed on November 20, 2025, [https://www.daydreamsoft.com/blog/voice-first-and-multimodal-interfaces-the-future-of-human-computer-interaction](https://www.daydreamsoft.com/blog/voice-first-and-multimodal-interfaces-the-future-of-human-computer-interaction)  
26. Story, accessed on November 20, 2025, [https://www.etri.re.kr/webzine/eng/20170728/sub03.html](https://www.etri.re.kr/webzine/eng/20170728/sub03.html)  
27. Invisible Design: When the Best UI is No UI at All \- iROID Technologies, accessed on November 20, 2025, [https://www.iroidtechnologies.com/blog/invisible-web-design](https://www.iroidtechnologies.com/blog/invisible-web-design)  
28. How to Reduce Cognitive Load for Voice Design \- Amazon Developers, accessed on November 20, 2025, [https://developer.amazon.com/en-US/blogs/alexa/alexa-skills-kit/2019/08/how-to-reduce-cognitive-load-for-voice-design](https://developer.amazon.com/en-US/blogs/alexa/alexa-skills-kit/2019/08/how-to-reduce-cognitive-load-for-voice-design)  
29. Classifying human-AI agent interaction \- Red Hat, accessed on November 20, 2025, [https://www.redhat.com/en/blog/classifying-human-ai-agent-interaction](https://www.redhat.com/en/blog/classifying-human-ai-agent-interaction)  
30. Proactive Conversational Agents with Inner Thoughts \- arXiv, accessed on November 20, 2025, [https://arxiv.org/html/2501.00383v2](https://arxiv.org/html/2501.00383v2)  
31. Sensible Agent: A Framework for Unobtrusive Interaction with Proactive AR Agents \- arXiv, accessed on November 20, 2025, [https://arxiv.org/html/2509.09255v2](https://arxiv.org/html/2509.09255v2)  
32. Maintaining Engagement in Long-term Interventions with Relational Agents \- PMC \- NIH, accessed on November 20, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC3035950/](https://pmc.ncbi.nlm.nih.gov/articles/PMC3035950/)  
33. Establishing and Maintaining Long-Term Human- Computer Relationships, accessed on November 20, 2025, [https://www.ccs.neu.edu/home/bickmore/publications/toCHI.pdf](https://www.ccs.neu.edu/home/bickmore/publications/toCHI.pdf)  
34. UbiComp '24: Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing, accessed on November 20, 2025, [https://www.ubicomp.org/ubicomp-iswc-2024/wp-content/uploads/2024/09/companion\_toc.html](https://www.ubicomp.org/ubicomp-iswc-2024/wp-content/uploads/2024/09/companion_toc.html)  
35. EarComp 2023 \- Earable Computing, accessed on November 20, 2025, [https://www.esense.io/earcomp2023/](https://www.esense.io/earcomp2023/)  
36. OpenEarable 2.0: Open-Source Earphone Platform for Physiological Ear Sensing \- DSpace@MIT, accessed on November 20, 2025, [https://dspace.mit.edu/bitstream/handle/1721.1/159051/3712069.pdf?sequence=1\&isAllowed=y](https://dspace.mit.edu/bitstream/handle/1721.1/159051/3712069.pdf?sequence=1&isAllowed=y)  
37. A Survey of Earable Technology: Trends, Tools, and the Road Ahead \- arXiv, accessed on November 20, 2025, [https://arxiv.org/html/2506.05720v1](https://arxiv.org/html/2506.05720v1)  
38. EarXplore \- Earable Interaction Database, accessed on November 20, 2025, [https://earxplore.teco.edu/](https://earxplore.teco.edu/)  
39. Exploring User Defined Gestures for Ear-Based Interactions | Request PDF \- ResearchGate, accessed on November 20, 2025, [https://www.researchgate.net/publication/347484030\_Exploring\_User\_Defined\_Gestures\_for\_Ear-Based\_Interactions](https://www.researchgate.net/publication/347484030_Exploring_User_Defined_Gestures_for_Ear-Based_Interactions)  
40. Whispering Wearables: Multimodal Approach to Silent Speech Recognition with Head-Worn Devices \- Microsoft, accessed on November 20, 2025, [https://www.microsoft.com/en-us/research/wp-content/uploads/2024/11/3678957.3685720.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2024/11/3678957.3685720.pdf)  
41. New Ears: An Exploratory Study of Audio Interaction Techniques for Performing Search in a Virtual Reality Environment \- Muzhe Wu, accessed on November 20, 2025, [https://wumuzhe.com/paper/new-ears-ismar24.pdf](https://wumuzhe.com/paper/new-ears-ismar24.pdf)  
42. Voice AI \- Strange Ventures, accessed on November 20, 2025, [https://www.strangevc.com/stories/a-living-research-report-voice-ai](https://www.strangevc.com/stories/a-living-research-report-voice-ai)  
43. Uncanny valley \- Wikipedia, accessed on November 20, 2025, [https://en.wikipedia.org/wiki/Uncanny\_valley](https://en.wikipedia.org/wiki/Uncanny_valley)  
44. A mismatch in the human realism of face and voice produces an uncanny valley \- PMC \- NIH, accessed on November 20, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC3485769/](https://pmc.ncbi.nlm.nih.gov/articles/PMC3485769/)  
45. Crossing the uncanny valley of conversational voice \- Sesame, accessed on November 20, 2025, [https://www.sesame.com/research/crossing\_the\_uncanny\_valley\_of\_voice](https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice)  
46. System architecture of the MPEG-DASH live video streaming over 5G with MEC servers \- ResearchGate, accessed on November 20, 2025, [https://www.researchgate.net/figure/System-architecture-of-the-MPEG-DASH-live-video-streaming-over-5G-with-MEC-servers\_fig1\_345245856](https://www.researchgate.net/figure/System-architecture-of-the-MPEG-DASH-live-video-streaming-over-5G-with-MEC-servers_fig1_345245856)  
47. Zero UI: Designing for Screenless Interactions \- the Adobe Blog, accessed on November 20, 2025, [https://blog.adobe.com/en/publish/2017/01/19/zero-ui-designing-for-screen-less-interactions](https://blog.adobe.com/en/publish/2017/01/19/zero-ui-designing-for-screen-less-interactions)  
48. Audio Augmented Reality Applications \- Emergent Mind, accessed on November 20, 2025, [https://www.emergentmind.com/topics/audio-augmented-reality-aar-applications](https://www.emergentmind.com/topics/audio-augmented-reality-aar-applications)  
49. AudioMiXR: Spatial Audio Object Manipulation with 6DoF for Sound Design in Augmented Reality \- arXiv, accessed on November 20, 2025, [https://arxiv.org/html/2502.02929v4](https://arxiv.org/html/2502.02929v4)  
50. Hear We Are: Spatial Audio Benefits Perceptions of Turn-Taking and Social Presence in Video Meetings | Request PDF \- ResearchGate, accessed on November 20, 2025, [https://www.researchgate.net/publication/374079743\_Hear\_We\_Are\_Spatial\_Audio\_Benefits\_Perceptions\_of\_Turn-Taking\_and\_Social\_Presence\_in\_Video\_Meetings](https://www.researchgate.net/publication/374079743_Hear_We_Are_Spatial_Audio_Benefits_Perceptions_of_Turn-Taking_and_Social_Presence_in_Video_Meetings)  
51. From Chaos to Context: How Knowledge Graphs and Agentic Workflows Are Reimagining Unstructured Data \- Dimension Labs, accessed on November 20, 2025, [https://www.dimensionlabs.io/blog/how-knowledge-graphs-and-agentic-workflows](https://www.dimensionlabs.io/blog/how-knowledge-graphs-and-agentic-workflows)  
52. 6 Steps to Build Knowledge Graphs from Slack Conversations and Internal Communications, accessed on November 20, 2025, [https://www.datagrid.com/blog/build-knowledge-graphs-slack-conversations-ai-agents-d16d8](https://www.datagrid.com/blog/build-knowledge-graphs-slack-conversations-ai-agents-d16d8)  
53. A Graph-Based Approach for Conversational AI-Driven Personal Memory Capture and Retrieval in a Real-world Application \- arXiv, accessed on November 20, 2025, [https://arxiv.org/html/2412.05447v1](https://arxiv.org/html/2412.05447v1)  
54. Dynamic Knowledge Graph Alignment, accessed on November 20, 2025, [https://ojs.aaai.org/index.php/AAAI/article/view/16585/16392](https://ojs.aaai.org/index.php/AAAI/article/view/16585/16392)  
55. Glance builds Gemini-powered Knowledge Graph with Google Cloud, accessed on November 20, 2025, [https://cloud.google.com/blog/topics/customers/glance-builds-gemini-powered-knowledge-graph-with-google-cloud](https://cloud.google.com/blog/topics/customers/glance-builds-gemini-powered-knowledge-graph-with-google-cloud)  
56. CLARE: Context-Aware, Interactive Knowledge Graph Construction from Transcripts \- MDPI, accessed on November 20, 2025, [https://www.mdpi.com/2078-2489/16/10/866](https://www.mdpi.com/2078-2489/16/10/866)  
57. Building a Biomimetic Memory System for Claude in 2 Hours (No Code Required) \- Reddit, accessed on November 20, 2025, [https://www.reddit.com/r/claudexplorers/comments/1oq0bga/building\_a\_biomimetic\_memory\_system\_for\_claude\_in/](https://www.reddit.com/r/claudexplorers/comments/1oq0bga/building_a_biomimetic_memory_system_for_claude_in/)  
58. Memory and mental time travel in humans and social robots \- PMC \- PubMed Central, accessed on November 20, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC6452248/](https://pmc.ncbi.nlm.nih.gov/articles/PMC6452248/)  
59. Embodiment and Human-Inspired Socio-Cognitive Mechanisms in Artificial Agents: A Systematic Scoping Review, accessed on November 20, 2025, [https://repository.tudelft.nl/file/File\_7915f0bc-d68c-4b33-8cd9-3c0ef4a179b1?preview=1](https://repository.tudelft.nl/file/File_7915f0bc-d68c-4b33-8cd9-3c0ef4a179b1?preview=1)  
60. (PDF) Memory and mental time travel in humans and social robots \- ResearchGate, accessed on November 20, 2025, [https://www.researchgate.net/publication/330737808\_Memory\_and\_mental\_time\_travel\_in\_humans\_and\_social\_robots](https://www.researchgate.net/publication/330737808_Memory_and_mental_time_travel_in_humans_and_social_robots)  
61. Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics \- ACL Anthology, accessed on November 20, 2025, [https://aclanthology.org/2024.naacl-demo.pdf](https://aclanthology.org/2024.naacl-demo.pdf)  
62. Fuzzy Memory Networks and Contextual Schemas: Enhancing ChatGPT Responses in a Personalized Educational System \- MDPI, accessed on November 20, 2025, [https://www.mdpi.com/2073-431X/14/3/89](https://www.mdpi.com/2073-431X/14/3/89)  
63. The Agent's Memory Dilemma: Is Forgetting a Bug or a Feature? | by Tao An \- Medium, accessed on November 20, 2025, [https://medium.com/@tao-hpu/the-agents-memory-dilemma-is-forgetting-a-bug-or-a-feature-a7e8421793d4](https://medium.com/@tao-hpu/the-agents-memory-dilemma-is-forgetting-a-bug-or-a-feature-a7e8421793d4)  
64. Forgetting in AI Agent Memory Systems | by Volodymyr Pavlyshyn, accessed on November 20, 2025, [https://ai.plainenglish.io/forgetting-in-ai-agent-memory-systems-7049181798c4](https://ai.plainenglish.io/forgetting-in-ai-agent-memory-systems-7049181798c4)  
65. ciu: Contextual Importance and Utility, accessed on November 20, 2025, [https://cran.r-project.org/web/packages/ciu/ciu.pdf](https://cran.r-project.org/web/packages/ciu/ciu.pdf)  
66. KaryFramling/ciu: R implementation of Contextual Importance and Utility for Explainable AI \- GitHub, accessed on November 20, 2025, [https://github.com/KaryFramling/ciu](https://github.com/KaryFramling/ciu)  
67. “Having Lunch Now”: Understanding How Users Engage with a Proactive Agent for Daily Planning and Self-Reflection \- arXiv, accessed on November 20, 2025, [https://arxiv.org/html/2509.24073v1](https://arxiv.org/html/2509.24073v1)  
68. Full article: AI companions for lonely individuals and the role of social presence, accessed on November 20, 2025, [https://www.tandfonline.com/doi/full/10.1080/08824096.2022.2045929](https://www.tandfonline.com/doi/full/10.1080/08824096.2022.2045929)  
69. When AI companions for lonely people seem a bit too human \- Ohio State News, accessed on November 20, 2025, [https://news.osu.edu/when-ai-companions-for-lonely-people-seem-a-bit-too-human/](https://news.osu.edu/when-ai-companions-for-lonely-people-seem-a-bit-too-human/)  
70. The Parasocial Paradox: Why AI Podcast Hosts Might Actually Work (But Not How You Think) \- Sounds Profitable, accessed on November 20, 2025, [https://soundsprofitable.com/article/the-parasocial-paradox/](https://soundsprofitable.com/article/the-parasocial-paradox/)  
71. When Human-AI Interactions Become Parasocial: Agency and Anthropomorphism in Affective Design \- ACM FAccT, accessed on November 20, 2025, [https://facctconference.org/static/papers24/facct24-71.pdf](https://facctconference.org/static/papers24/facct24-71.pdf)  
72. Zero UI to Help the Elderly \- Theseus, accessed on November 20, 2025, [https://www.theseus.fi/bitstream/handle/10024/810269/Shah\_Ruchi.pdf?sequence=2\&isAllowed=y](https://www.theseus.fi/bitstream/handle/10024/810269/Shah_Ruchi.pdf?sequence=2&isAllowed=y)  
73. The Design of Everyday Things \- Wikipedia, accessed on November 20, 2025, [https://en.wikipedia.org/wiki/The\_Design\_of\_Everyday\_Things](https://en.wikipedia.org/wiki/The_Design_of_Everyday_Things)  
74. UX design guidelines for audio/sound feedback and interaction of UI \- UX Stack Exchange, accessed on November 20, 2025, [https://ux.stackexchange.com/questions/122026/ux-design-guidelines-for-audio-sound-feedback-and-interaction-of-ui](https://ux.stackexchange.com/questions/122026/ux-design-guidelines-for-audio-sound-feedback-and-interaction-of-ui)  
75. Auditory Feedback in Integrated Development Environments to Provide Cognitive Support for Novice Developers \- NSUWorks, accessed on November 20, 2025, [https://nsuworks.nova.edu/cgi/viewcontent.cgi?article=2148\&context=gscis\_etd](https://nsuworks.nova.edu/cgi/viewcontent.cgi?article=2148&context=gscis_etd)  
76. 5\. Auditory Interfaces, accessed on November 20, 2025, [https://www.cmu.edu/dietrich/psychology/shinn/publications/pdfs/2007/2007auditoryinterfaces\_peres.pdf](https://www.cmu.edu/dietrich/psychology/shinn/publications/pdfs/2007/2007auditoryinterfaces_peres.pdf)  
77. 4 Loading Micro-Interactions that Improve UX | by Ashish Garg | Bootcamp | Medium, accessed on November 20, 2025, [https://medium.com/design-bootcamp/4-loading-micro-interactions-that-improve-ux-c2cd8851d90b](https://medium.com/design-bootcamp/4-loading-micro-interactions-that-improve-ux-c2cd8851d90b)  
78. Voice User Interface (VUI) Design Principles: Guide (2025) \- Parallel HQ, accessed on November 20, 2025, [https://www.parallelhq.com/blog/voice-user-interface-vui-design-principles](https://www.parallelhq.com/blog/voice-user-interface-vui-design-principles)  
79. Multimodal Interaction, Interfaces, and Communication: A Survey \- MDPI, accessed on November 20, 2025, [https://www.mdpi.com/2414-4088/9/1/6](https://www.mdpi.com/2414-4088/9/1/6)  
80. A personal assistant for everyone: The promise of ambient AI \- Freethink, accessed on November 20, 2025, [https://www.freethink.com/artificial-intelligence/ambient-ai](https://www.freethink.com/artificial-intelligence/ambient-ai)  
81. Surveillance, Capitalism, Leisure, and Data: Being Watched, Giving, Becoming \- University of Waterloo, accessed on November 20, 2025, [https://uwaterloo.ca/scholar/sites/ca.scholar/files/l2cousin/files/2023\_-\_cousineau\_kumm\_schultz\_-\_surveillance\_capitalism\_leisure\_and\_data\_-\_being\_watched\_giving\_becoming.pdf](https://uwaterloo.ca/scholar/sites/ca.scholar/files/l2cousin/files/2023_-_cousineau_kumm_schultz_-_surveillance_capitalism_leisure_and_data_-_being_watched_giving_becoming.pdf)  
82. Data Colonialism → Area → Resource 76, accessed on November 20, 2025, [https://prism.sustainability-directory.com/area/data-colonialism/resource/76/](https://prism.sustainability-directory.com/area/data-colonialism/resource/76/)  
83. The Costs of Connection \- Melbourne Law School, accessed on November 20, 2025, [https://law.unimelb.edu.au/\_\_data/assets/pdf\_file/0008/3290381/Couldry-and-Mejias-Preface-and-Ch-1.pdf](https://law.unimelb.edu.au/__data/assets/pdf_file/0008/3290381/Couldry-and-Mejias-Preface-and-Ch-1.pdf)  
84. IXDA Budapest 2020: Design Ethics? No Thanks\! \- New Design Congress, accessed on November 20, 2025, [https://newdesigncongress.org/en/stream/2020/design-ethics-no-thanks/](https://newdesigncongress.org/en/stream/2020/design-ethics-no-thanks/)  
85. The Silent Erosion: How AI's Helping Hand Weakens Our Mental Grip, accessed on November 20, 2025, [https://www.cigionline.org/articles/the-silent-erosion-how-ais-helping-hand-weakens-our-mental-grip/](https://www.cigionline.org/articles/the-silent-erosion-how-ais-helping-hand-weakens-our-mental-grip/)  
86. The Shepherd Test: How Will Super Intelligent Agents Balance Care and Control in Asymmetric Relationships? \- arXiv, accessed on November 20, 2025, [https://arxiv.org/html/2506.01813v4](https://arxiv.org/html/2506.01813v4)  
87. The Dark Side of User Interface Design \- urbanNext, accessed on November 20, 2025, [https://urbannext.net/the-dark-side/](https://urbannext.net/the-dark-side/)  
88. The Artificial Intelligence illusion: How invisible workers fuel the "automated" economy, accessed on November 20, 2025, [https://www.ilo.org/resource/article/artificial-intelligence-illusion-how-invisible-workers-fuel-automated](https://www.ilo.org/resource/article/artificial-intelligence-illusion-how-invisible-workers-fuel-automated)  
89. The Myth of Inevitable AI \- Leon Furze, accessed on November 20, 2025, [https://leonfurze.com/2025/04/28/the-myth-of-inevitable-ai/](https://leonfurze.com/2025/04/28/the-myth-of-inevitable-ai/)  
90. The Evolution of User Interface Design \- Communications of the ACM, accessed on November 20, 2025, [https://cacm.acm.org/blogcacm/the-evolution-of-user-interface-design/](https://cacm.acm.org/blogcacm/the-evolution-of-user-interface-design/)
91. Cochlear Implants \- NIDCD (NIH), accessed on November 20, 2025, [https://www.nidcd.nih.gov/health/cochlear-implants](https://www.nidcd.nih.gov/health/cochlear-implants)  
92. Cochlear Implants \- U.S. FDA, accessed on November 20, 2025, [https://www.fda.gov/medical-devices/implants-and-prosthetics/cochlear-implants](https://www.fda.gov/medical-devices/implants-and-prosthetics/cochlear-implants)  
93. Cochlear Implants and MRI Safety \- U.S. FDA, accessed on November 20, 2025, [https://www.fda.gov/medical-devices/cochlear-implants/cochlear-implants-and-mri-safety](https://www.fda.gov/medical-devices/cochlear-implants/cochlear-implants-and-mri-safety)  
94. Bone Conduction Implants \- Cochlear, accessed on November 20, 2025, [https://www.cochlear.com/us/en/home/hearing-solutions/bone-conduction-implants](https://www.cochlear.com/us/en/home/hearing-solutions/bone-conduction-implants)  
95. ClinicalTrials.gov: NCT05035823 (Synchron Stentrode) \- NIH, accessed on November 20, 2025, [https://clinicaltrials.gov/study/NCT05035823](https://clinicaltrials.gov/study/NCT05035823)  
96. Technology \- Synchron, accessed on November 20, 2025, [https://synchron.com/technology/](https://synchron.com/technology/)  
97. PRIME Study Progress Update \- Neuralink, accessed on November 20, 2025, [https://neuralink.com/updates/prime-study-progress-update/](https://neuralink.com/updates/prime-study-progress-update/)  
98. Utah Array \- Blackrock Neurotech, accessed on November 20, 2025, [https://blackrockneurotech.com/products/utah-array/](https://blackrockneurotech.com/products/utah-array/)  
99. Nature Biotechnology article on chronic implanted BCIs \- Nature Biotechnology, accessed on November 20, 2025, [https://www.nature.com/articles/s41587-021-01017-8](https://www.nature.com/articles/s41587-021-01017-8)  
100. Nature Biomedical Engineering article on BCI safety/efficacy \- Nature Biomedical Engineering, accessed on November 20, 2025, [https://www.nature.com/articles/s41551-021-00738-4](https://www.nature.com/articles/s41551-021-00738-4)  
101. Walletmor NFC Implants \- Walletmor, accessed on November 20, 2025, [https://walletmor.com/](https://walletmor.com/)
102. World report on hearing \- World Health Organization, 2021, accessed on November 20, 2025, [https://www.who.int/publications/i/item/9789240020481](https://www.who.int/publications/i/item/9789240020481)  
103. Cochlear implants for children and adults with severe to profound deafness (TA566) \- NICE, accessed on November 20, 2025, [https://www.nice.org.uk/guidance/ta566](https://www.nice.org.uk/guidance/ta566)  
104. Regulation (EU) 2017/745 on medical devices (MDR) \- EUR-Lex, accessed on November 20, 2025, [https://eur-lex.europa.eu/eli/reg/2017/745/oj](https://eur-lex.europa.eu/eli/reg/2017/745/oj)  
105. Safety guidelines for magnetic resonance imaging (MRI) equipment in clinical use \- UK MHRA, accessed on November 20, 2025, [https://www.gov.uk/government/publications/safety-guidelines-for-magnetic-resonance-imaging-mri-equipment-in-clinical-use](https://www.gov.uk/government/publications/safety-guidelines-for-magnetic-resonance-imaging-mri-equipment-in-clinical-use)  
106. Recommendation of the Council on Responsible Innovation in Neurotechnology \- OECD, accessed on November 20, 2025, [https://legalinstruments.oecd.org/en/instruments/OECD-LEGAL-0457](https://legalinstruments.oecd.org/en/instruments/OECD-LEGAL-0457)  
107. Novel neurotechnologies: intervening in the brain \- Nuffield Council on Bioethics, accessed on November 20, 2025, [https://www.nuffieldbioethics.org/publications/novel-neurotechnologies](https://www.nuffieldbioethics.org/publications/novel-neurotechnologies)  
108. Medical Device Regulations and Review (Overview) \- PMDA (Japan), accessed on November 20, 2025, [https://www.pmda.go.jp/english/review-services/medical-devices/0001.html](https://www.pmda.go.jp/english/review-services/medical-devices/0001.html)  
109. How we regulate medical devices \- Therapeutic Goods Administration (Australia), accessed on November 20, 2025, [https://www.tga.gov.au/how-we-regulate/medical-devices](https://www.tga.gov.au/how-we-regulate/medical-devices)  
110. Global report on assistive technology \- World Health Organization, 2022, accessed on November 20, 2025, [https://www.who.int/publications/i/item/9789240049451](https://www.who.int/publications/i/item/9789240049451)